{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AirNow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primary Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T16:15:43.827074Z",
     "start_time": "2021-05-14T16:15:43.816839Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "import PyPDF2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:18:28.407296Z",
     "start_time": "2021-05-13T15:18:28.403081Z"
    }
   },
   "outputs": [],
   "source": [
    "fileproducts_url = 'http://files.airnowtech.org'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:http://files.airnowtech.org/?prefix=airnow/2012/20120101/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "daily_data.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "daily_data_v2.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:18:35.506554Z",
     "start_time": "2021-05-13T15:18:35.504715Z"
    }
   },
   "outputs": [],
   "source": [
    "# http://files.airnowtech.org/?prefix=airnow/{year}/{year}{month}{day}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:33:45.413712Z",
     "start_time": "2021-05-13T16:33:45.411822Z"
    }
   },
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(open(\"http://files.airnowtech.org/?prefix=airnow/2013/20131231/daily_data.dat\"), \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:33:51.869670Z",
     "start_time": "2021-05-13T16:33:51.867743Z"
    }
   },
   "outputs": [],
   "source": [
    "# imported the requests library\n",
    "import requests\n",
    "data_url = \"http://files.airnowtech.org/?prefix=airnow/2013/20131231/daily_data.dat\"\n",
    "\n",
    "year = '2013'\n",
    "month = '12'\n",
    "day = '31'\n",
    "    \n",
    "    \n",
    "# URL of the image to be downloaded is defined as image_url\n",
    "r = requests.get(data_url) # create HTTP response object\n",
    "  \n",
    "# send a HTTP request to the server and save\n",
    "# the HTTP response in a response object called r\n",
    "with open(\"daily_data.dat\",'wb') as f:\n",
    "  \n",
    "    # Saving received content as a png file in\n",
    "    # binary format\n",
    "  \n",
    "    # write the contents of the response (r.content)\n",
    "    # to a new file in binary mode.\n",
    "    \n",
    "       \n",
    "    f.write(r.content, f'../../data/air_q/product_files/{year}/{year}{month}{day}.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T20:15:17.066223Z",
     "start_time": "2021-05-14T18:14:28.440923Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report = []\n",
    "\n",
    "# establish date range\n",
    "start_date = datetime.date(2013, 10, 20)    # 2013, 10, 20\n",
    "end_date = datetime.date(2021, 5, 31)\n",
    "delta = datetime.timedelta(days=1)\n",
    "\n",
    "# begin loop through date range\n",
    "while start_date <= end_date:\n",
    "    \n",
    "    # create strings of start and end dates\n",
    "    start_string = str(start_date)\n",
    "    end_string = str(end_date)\n",
    "    \n",
    "    # create substrings for year, month, day\n",
    "    year = start_string[0:4]\n",
    "    month = start_string[5:7]\n",
    "    day = start_string[8:10]\n",
    "\n",
    "    # create filepath string\n",
    "    date_string = f'{year}/{year}{month}{day}/'\n",
    "\n",
    "    # loop through two possible filenames\n",
    "    for file in ['daily_data.dat', 'daily_data_v2.dat']:\n",
    "         \n",
    "            \n",
    "        try:\n",
    "            # set full url for data file\n",
    "            data_url = f' https://s3-us-west-1.amazonaws.com//files.airnowtech.org/airnow/{date_string}{file}'\n",
    "\n",
    "            # get request for each day's data file\n",
    "            r = requests.get(data_url)\n",
    "        \n",
    "            if r.status_code == 200:\n",
    "        \n",
    "                file_path = f\"data/{file}\"\n",
    "                directory = os.path.dirname(file_path)\n",
    "\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)       \n",
    "\n",
    "                with open(f'data/{date_string[5:-1]}{file}','a') as f:\n",
    "                    f.write(r.text)\n",
    "                \n",
    "                cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                report.append({'event': f'Data saved successfully',\n",
    "                               'date': f'{year}-{month}-{day}',\n",
    "                               'file': f'{file}',\n",
    "                               'datetime': f'{cur_datetime}',\n",
    "                               'exception': ''\n",
    "                               })\n",
    "                else:\n",
    "                    pass\n",
    "        \n",
    "        except Exception as e:\n",
    "            cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f'{cur_datetime}: {e}')\n",
    "            report.append({'event': f'Could not save data',\n",
    "                           'date': f'{year}-{month}-{day}',\n",
    "                           'file': f'{file}',\n",
    "                           'datetime': f'{cur_datetime}',\n",
    "                           'exception': f'{e}'\n",
    "                           })            \n",
    "            \n",
    "            \n",
    "        time.sleep(.25)            \n",
    "        df_report = pd.DataFrame(report)\n",
    "        df_report.to_csv(f'report.csv', index=False)    \n",
    "\n",
    "    start_date += delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T16:23:13.113908Z",
     "start_time": "2021-05-14T16:23:13.103132Z"
    }
   },
   "outputs": [],
   "source": [
    "file_test = '/Users/rileydrobertson/DSI/projects/project_5/AirQuality-USWest/code/rr/data/daily_data.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T16:24:25.454790Z",
     "start_time": "2021-05-14T16:24:25.419064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/04/13</td>\n",
       "      <td>000020301</td>\n",
       "      <td>WELLINGTON</td>\n",
       "      <td>PM2.5-24hr</td>\n",
       "      <td>UG/M3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>24</td>\n",
       "      <td>Environment Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/04/13</td>\n",
       "      <td>000020301</td>\n",
       "      <td>WELLINGTON</td>\n",
       "      <td>OZONE-1HR</td>\n",
       "      <td>PPB</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Environment Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/04/13</td>\n",
       "      <td>000020301</td>\n",
       "      <td>WELLINGTON</td>\n",
       "      <td>OZONE-8HR</td>\n",
       "      <td>PPB</td>\n",
       "      <td>33.0</td>\n",
       "      <td>8</td>\n",
       "      <td>Environment Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/04/13</td>\n",
       "      <td>000030118</td>\n",
       "      <td>HALIFAX</td>\n",
       "      <td>OZONE-8HR</td>\n",
       "      <td>PPB</td>\n",
       "      <td>28.0</td>\n",
       "      <td>8</td>\n",
       "      <td>Environment Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/04/13</td>\n",
       "      <td>000030118</td>\n",
       "      <td>HALIFAX</td>\n",
       "      <td>OZONE-1HR</td>\n",
       "      <td>PPB</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Environment Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3106</th>\n",
       "      <td>12/04/13</td>\n",
       "      <td>371050002</td>\n",
       "      <td>Blackstone</td>\n",
       "      <td>OZONE-8HR</td>\n",
       "      <td>PPB</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8</td>\n",
       "      <td>North Carolina DENR - Divison of Air Quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>12/04/13</td>\n",
       "      <td>020900035</td>\n",
       "      <td>North Pole Fire Stat</td>\n",
       "      <td>PM2.5-24hr</td>\n",
       "      <td>UG/M3</td>\n",
       "      <td>67.7</td>\n",
       "      <td>24</td>\n",
       "      <td>State of Alaska DEC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3108</th>\n",
       "      <td>12/04/13</td>\n",
       "      <td>020900039</td>\n",
       "      <td>Watershed School</td>\n",
       "      <td>PM2.5-24hr</td>\n",
       "      <td>UG/M3</td>\n",
       "      <td>36.3</td>\n",
       "      <td>24</td>\n",
       "      <td>State of Alaska DEC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3109</th>\n",
       "      <td>12/04/13</td>\n",
       "      <td>020200044</td>\n",
       "      <td>Tudor</td>\n",
       "      <td>PM10-24hr</td>\n",
       "      <td>UG/M3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24</td>\n",
       "      <td>State of Alaska DEC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3110</th>\n",
       "      <td>12/04/13</td>\n",
       "      <td>160830007</td>\n",
       "      <td>Rock Creek PM2.5</td>\n",
       "      <td>PM2.5-24hr</td>\n",
       "      <td>UG/M3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>24</td>\n",
       "      <td>Idaho Department of Environmental Quality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3111 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1                     2           3      4     5   6  \\\n",
       "0     12/04/13  000020301            WELLINGTON  PM2.5-24hr  UG/M3   4.3  24   \n",
       "1     12/04/13  000020301            WELLINGTON   OZONE-1HR    PPB  34.0   1   \n",
       "2     12/04/13  000020301            WELLINGTON   OZONE-8HR    PPB  33.0   8   \n",
       "3     12/04/13  000030118               HALIFAX   OZONE-8HR    PPB  28.0   8   \n",
       "4     12/04/13  000030118               HALIFAX   OZONE-1HR    PPB  34.0   1   \n",
       "...        ...        ...                   ...         ...    ...   ...  ..   \n",
       "3106  12/04/13  371050002            Blackstone   OZONE-8HR    PPB  31.0   8   \n",
       "3107  12/04/13  020900035  North Pole Fire Stat  PM2.5-24hr  UG/M3  67.7  24   \n",
       "3108  12/04/13  020900039      Watershed School  PM2.5-24hr  UG/M3  36.3  24   \n",
       "3109  12/04/13  020200044                 Tudor   PM10-24hr  UG/M3  18.0  24   \n",
       "3110  12/04/13  160830007      Rock Creek PM2.5  PM2.5-24hr  UG/M3   3.5  24   \n",
       "\n",
       "                                                 7  \n",
       "0                               Environment Canada  \n",
       "1                               Environment Canada  \n",
       "2                               Environment Canada  \n",
       "3                               Environment Canada  \n",
       "4                               Environment Canada  \n",
       "...                                            ...  \n",
       "3106  North Carolina DENR - Divison of Air Quality  \n",
       "3107                           State of Alaska DEC  \n",
       "3108                           State of Alaska DEC  \n",
       "3109                           State of Alaska DEC  \n",
       "3110     Idaho Department of Environmental Quality  \n",
       "\n",
       "[3111 rows x 8 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(file_test, sep='|', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**Alert Code for main API Request function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "I found a python module online that will notify me via Slack when my functions are complete.\n",
    "\n",
    "Source: https://github.com/huggingface/knockknock#slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.149031Z",
     "start_time": "2021-05-06T03:46:02.789702Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import knockknock\n",
    "kk_url = \"https://hooks.slack.com/services/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\"\n",
    "kk_channel_name = 'jupyter-notebook'\n",
    "kk_users = ['rileyrobertsond']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Subreddit info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**NFL** - https://www.reddit.com/r/nfl\n",
    "\n",
    "**Premier League** - https://www.reddit.com/r/PremierLeague\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.153919Z",
     "start_time": "2021-05-06T03:46:03.151379Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "nfl = 'nfl'\n",
    "epl = 'PremierLeague'\n",
    "\n",
    "subs = [nfl, epl]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Single Request Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.162098Z",
     "start_time": "2021-05-06T03:46:03.157031Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Using Pushshift API (https://pushshift.io/api-parameters)\n",
    "# returns a list of dictionaries from chosen subreddit\n",
    "# each dictionary containing 1 reddit submission (post) or comment\n",
    "\n",
    "def get_request(dict_params, request_type='submission'):\n",
    "    if request_type == 'submission':\n",
    "        url = f'https://api.pushshift.io/reddit/{request_type}/search'\n",
    "        res = requests.get(url, dict_params)  \n",
    "        return res.json()['data']\n",
    "\n",
    "    if request_type == 'comment':\n",
    "        return 'Comment scraping development in progress'  # res.json()['data']\n",
    "\n",
    "    else:\n",
    "        return 'Enter valid request_type (submission or comment)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T15:40:27.280161Z",
     "start_time": "2021-04-28T15:40:27.274931Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**Submission (post) test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.676744Z",
     "start_time": "2021-05-06T03:46:03.165095Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# parameters setup\n",
    "params = {'subreddit': epl, 'size': '1', 'is_self': True}\n",
    "\n",
    "# assignment of function return to variable\n",
    "get_req = get_request(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.699385Z",
     "start_time": "2021-05-06T03:46:03.682464Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "subbreddit: PremierLeague\n",
      "     title: When will Premier League teams figure out Thomas T...\n",
      "    author: Lersbyte\n",
      "   created: 1620259116\n",
      "  comments: 0\n",
      "       url: https://www.reddit.com/r/PremierLeague/comments/n5uajf/when_will_premier_league_teams_figure_out_thomas/\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "f'''\n",
    "subbreddit: {get_req[0]['subreddit']}\n",
    "     title: {get_req[0]['title'][:50]}...\n",
    "    author: {get_req[0]['author']}\n",
    "   created: {get_req[0]['created_utc']}\n",
    "  comments: {get_req[0]['num_comments']}\n",
    "       url: {get_req[0]['url']}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Loop Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "I wanted to make the process of scraping as easy as possible, so I built a function that has evolved over the course of my work for OverArmor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.732027Z",
     "start_time": "2021-05-06T03:46:03.711384Z"
    },
    "code_folding": [
     21
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to make [n] requests of 100 posts. \n",
    "# Each request will be for a period of [win] days and the periods will not overlap\n",
    "# Exports .csv of API responses for each subreddit\n",
    "\n",
    "@knockknock.slack_sender(webhook_url=kk_url, channel=kk_channel_name, user_mentions=kk_users)\n",
    "def api_requests(n, win, subs_list, output_folder, size=100, is_self=True, is_video=False):\n",
    "\n",
    "    # For Loop to scrape multiple subreddits:\n",
    "    for sub in subs_list:\n",
    "        \n",
    "        # List instantiation for error/event logging\n",
    "        report = []\n",
    "        \n",
    "        # Onscreen display during loops:        \n",
    "        cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f'API request initiated. Scraping r/{sub} in progress...')\n",
    "        print(f'   Current time: {cur_datetime}')\n",
    "        print('')\n",
    "        \n",
    "        # Adding updates to event log\n",
    "        report.append({'event': f'API request from r/{sub} began',\n",
    "                       'datetime': f'{cur_datetime}',\n",
    "                       'exception': ''\n",
    "                       })\n",
    "\n",
    "        # Setting parameters to use in data requests from the API. Parameters mostly set by function args. \n",
    "        params = {\n",
    "            'subreddit': sub,\n",
    "            'size': size,\n",
    "            'is_self': is_self,\n",
    "            'is_video': is_video,\n",
    "            'selftext:not': '[removed]'      # thanks to Amanda for posting this in the groupwork channel\n",
    "        }\n",
    "\n",
    "        list_data = []\n",
    "        \n",
    "        for i in range(1, n+1):               # I used the demo notebook to figure out these time\n",
    "            params['after'] = f'{i * win}d'   # parameters and loop structure using n and day_window\n",
    "\n",
    "            # Try/Except to handle any errors in the get_requests\n",
    "            try:\n",
    "                new_data = get_request(params)\n",
    "\n",
    "            # Adding errors to event log if get_request fails\n",
    "            except Exception as e:                                              \n",
    "                cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                report.append({'event': f'REQUEST FAILED: {i}',\n",
    "                               'datetime': f'{cur_datetime}',\n",
    "                               'exception': f'{e}'\n",
    "                               })\n",
    "                \n",
    "                # Onscreen display - Failure of single request\n",
    "                print(f'Request failed: {i}')\n",
    "                print(f'  Current time: {cur_datetime}')\n",
    "                print('')\n",
    "                time.sleep(.25)\n",
    "                continue\n",
    "                \n",
    "            # Adding newly pulled data to list for eventual DataFrame conversion and export\n",
    "            list_data.extend(new_data)\n",
    "    \n",
    "            # Adding completion updates to event log\n",
    "            cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            report.append({'event': f'Request complete: {i}',\n",
    "                           'datetime': f'{cur_datetime}',\n",
    "                           'exception': ''\n",
    "                           })\n",
    "            \n",
    "            # Onscreen updates for every 10 requests to provide timing updates\n",
    "            if i % 5 == 0:\n",
    "                print(f'Request complete: {i}')\n",
    "                print(f'    Current time: {cur_datetime}')\n",
    "                print('')\n",
    "            df_report = pd.DataFrame(report)\n",
    "            df_report.to_csv(f'../git_ignore/output/report.csv', index=False)\n",
    "            time.sleep(.25)\n",
    "\n",
    "            \n",
    "        # Setting up list of desired features to keep in primary output. Others will be excluded:\n",
    "        features = ['subreddit', 'created_utc', 'author', 'num_comments',\n",
    "                    'score', 'is_self', 'link_flair_text','title', 'selftext', 'full_link']\n",
    "            \n",
    "        # creating two DataFrames from list_data after all requests have been made and completed \n",
    "        df_output = pd.DataFrame(list_data)\n",
    "        df_outputfull = pd.DataFrame(list_data)\n",
    "        \n",
    "        # filtering the primary DataFrame\n",
    "        df_output = df_output[features]\n",
    "        \n",
    "        # creating simple date and time columns using utc code\n",
    "        # https://docs.python.org/3/library/datetime.html#date-objects\n",
    "        # https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "        df_output['date'] = df_output['created_utc'].map(lambda x: dt.datetime.fromtimestamp(x).strftime('%Y-%m-%d'))\n",
    "        df_output['time'] = df_output['created_utc'].map(lambda x: dt.datetime.fromtimestamp(x).strftime('%H:%M:%S'))\n",
    "\n",
    "        # Exporting data with selected/filtered features\n",
    "        # using if/else statement to accomodate varied input formatting\n",
    "        df_output.reset_index(inplace=True)\n",
    "        cur_datetime = dt.datetime.now().strftime('%Y_%m_%d-%H_%M_%S')\n",
    "\n",
    "        if output_folder[-1] == '/':\n",
    "            df_output.to_csv(f'{output_folder}{cur_datetime}_data_{sub}.csv', index=False,)\n",
    "        else:\n",
    "            df_output.to_csv(f'{output_folder}/{cur_datetime}_data_{sub}.csv', index=False,)     \n",
    "            \n",
    "        # Adding last update and then exporting event log as a CSV report\n",
    "        # using if/else statement to accomodate varied input formatting\n",
    "        cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        report.append({'event': f'r/{sub} data files saved to output folder', 'datetime': f'{cur_datetime}'})\n",
    "\n",
    "        df_report = pd.DataFrame(report)\n",
    "        cur_datetime = dt.datetime.now().strftime('%Y_%m_%d-%H_%M_%S')\n",
    "        \n",
    "        if output_folder[-1] == '/':\n",
    "            df_report.to_csv(f'{output_folder}{cur_datetime}_report_{sub}.csv', index=False,)\n",
    "        else:\n",
    "            df_report.to_csv(f'{output_folder}/{cur_datetime}_report_{sub}.csv', index=False,)\n",
    "\n",
    "        report = []\n",
    "\n",
    "        # Onscreen display - completion of each subreddit scrape\n",
    "        cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f'r/{sub} data and report files saved to output folder')\n",
    "        print(f'   Current time: {cur_datetime}')\n",
    "        print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "305.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

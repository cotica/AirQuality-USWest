{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AirNow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primary Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:42:20.957003Z",
     "start_time": "2021-05-13T15:42:20.953825Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:18:28.407296Z",
     "start_time": "2021-05-13T15:18:28.403081Z"
    }
   },
   "outputs": [],
   "source": [
    "fileproducts_url = 'http://files.airnowtech.org'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:http://files.airnowtech.org/?prefix=airnow/2012/20120101/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "daily_data.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "daily_data_v2.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:18:35.506554Z",
     "start_time": "2021-05-13T15:18:35.504715Z"
    }
   },
   "outputs": [],
   "source": [
    "# http://files.airnowtech.org/?prefix=airnow/{year}/{year}{month}{day}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:33:45.413712Z",
     "start_time": "2021-05-13T16:33:45.411822Z"
    }
   },
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(open(\"http://files.airnowtech.org/?prefix=airnow/2013/20131231/daily_data.dat\"), \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:33:51.869670Z",
     "start_time": "2021-05-13T16:33:51.867743Z"
    }
   },
   "outputs": [],
   "source": [
    "# imported the requests library\n",
    "import requests\n",
    "data_url = \"http://files.airnowtech.org/?prefix=airnow/2013/20131231/daily_data.dat\"\n",
    "\n",
    "year = '2013'\n",
    "month = '12'\n",
    "day = '31'\n",
    "    \n",
    "    \n",
    "# URL of the image to be downloaded is defined as image_url\n",
    "r = requests.get(data_url) # create HTTP response object\n",
    "  \n",
    "# send a HTTP request to the server and save\n",
    "# the HTTP response in a response object called r\n",
    "with open(\"daily_data.dat\",'wb') as f:\n",
    "  \n",
    "    # Saving received content as a png file in\n",
    "    # binary format\n",
    "  \n",
    "    # write the contents of the response (r.content)\n",
    "    # to a new file in binary mode.\n",
    "    \n",
    "       \n",
    "    f.write(r.content, f'../../data/air_q/product_files/{year}/{year}{month}{day}.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:55:48.176646Z",
     "start_time": "2021-05-13T16:55:46.546202Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120101daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120101daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120102daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120102daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120103daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120103daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120104daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120104daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120105daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120105daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120106daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120106daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120107daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120107daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120108daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120108daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120109daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120109daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120110daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120110daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120111daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120111daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120112daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120112daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120113daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120113daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120114daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120114daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120115daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120115daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120116daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120116daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120117daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120117daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120118daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120118daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120119daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120119daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120120daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120120daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120121daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120121daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120122daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120122daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120123daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120123daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120124daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120124daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120125daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120125daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120126daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120126daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120127daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120127daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120128daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120128daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120129daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120129daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120130daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120130daily_data_v2.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120131daily_data.txt'\n",
      "[Errno 2] No such file or directory: '../../data/air_q/2012/20120131daily_data_v2.txt'\n"
     ]
    }
   ],
   "source": [
    "report = []\n",
    "\n",
    "# establish date range\n",
    "start_date = datetime.date(2012, 1, 1)\n",
    "end_date = datetime.date(2012, 1, 31)\n",
    "delta = datetime.timedelta(days=1)\n",
    "\n",
    "# begin loop through date range\n",
    "while start_date <= end_date:\n",
    "    \n",
    "    # create strings of start and end dates\n",
    "    start_string = str(start_date)\n",
    "    end_string = str(end_date)\n",
    "    \n",
    "    # create substrings for year, month, day\n",
    "    year = start_string[0:4]\n",
    "    month = start_string[5:7]\n",
    "    day = start_string[8:10]\n",
    "\n",
    "    # create filepath string\n",
    "    date_string = f'{year}/{year}{month}{day}'\n",
    "\n",
    "    # loop through two possible filenames\n",
    "    for file in ['daily_data.dat', 'daily_data_v2.dat']:\n",
    " \n",
    "        try:\n",
    "            # set full url for data file\n",
    "            data_url = f'http://files.airnowtech.org/?prefix=airnow/{date_string}/{file}'\n",
    "\n",
    "            # get request for each day's data file\n",
    "            r = requests.get(data_url)\n",
    "\n",
    "#             with open(f'{date_string}{file[:-4]}.txt','wb') as f:\n",
    "            with open(r'../../data/air_q/' + f'{date_string}{file[:-4]}.txt'):\n",
    "                f.write(r.content)\n",
    "        except Exception as e:\n",
    "            print(f'{e}')\n",
    "            continue\n",
    "\n",
    "    start_date += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                              \n",
    "                cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                report.append({'event': f'REQUEST FAILED: {i}',\n",
    "                               'datetime': f'{cur_datetime}',\n",
    "                               'exception': f'{e}'\n",
    "                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:20:31.739679Z",
     "start_time": "2021-05-13T16:20:31.737296Z"
    }
   },
   "outputs": [],
   "source": [
    "url_stem = 'http://files.airnowtech.org/?prefix=airnow/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:20:33.555199Z",
     "start_time": "2021-05-13T16:20:33.551838Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(\"../data/data_links.html\"), \"html.parser\")\n",
    "links = []\n",
    "for link in soup.find_all('a'):\n",
    "    links.append(link.get('href'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.date(2012, 1, 1)\n",
    "number_of_days = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = [(start_date + datetime.timedelta(days = day)).isoformat() for day in range(number_of_days)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:44:11.608798Z",
     "start_time": "2021-05-13T15:44:11.601717Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "descriptor 'date' of 'datetime.datetime' object needs an argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7f372bb15e20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: descriptor 'date' of 'datetime.datetime' object needs an argument"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**Alert Code for main API Request function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "I found a python module online that will notify me via Slack when my functions are complete.\n",
    "\n",
    "Source: https://github.com/huggingface/knockknock#slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.149031Z",
     "start_time": "2021-05-06T03:46:02.789702Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import knockknock\n",
    "kk_url = \"https://hooks.slack.com/services/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\"\n",
    "kk_channel_name = 'jupyter-notebook'\n",
    "kk_users = ['rileyrobertsond']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Subreddit info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**NFL** - https://www.reddit.com/r/nfl\n",
    "\n",
    "**Premier League** - https://www.reddit.com/r/PremierLeague\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.153919Z",
     "start_time": "2021-05-06T03:46:03.151379Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "nfl = 'nfl'\n",
    "epl = 'PremierLeague'\n",
    "\n",
    "subs = [nfl, epl]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Single Request Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.162098Z",
     "start_time": "2021-05-06T03:46:03.157031Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Using Pushshift API (https://pushshift.io/api-parameters)\n",
    "# returns a list of dictionaries from chosen subreddit\n",
    "# each dictionary containing 1 reddit submission (post) or comment\n",
    "\n",
    "def get_request(dict_params, request_type='submission'):\n",
    "    if request_type == 'submission':\n",
    "        url = f'https://api.pushshift.io/reddit/{request_type}/search'\n",
    "        res = requests.get(url, dict_params)  \n",
    "        return res.json()['data']\n",
    "\n",
    "    if request_type == 'comment':\n",
    "        return 'Comment scraping development in progress'  # res.json()['data']\n",
    "\n",
    "    else:\n",
    "        return 'Enter valid request_type (submission or comment)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T15:40:27.280161Z",
     "start_time": "2021-04-28T15:40:27.274931Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**Submission (post) test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.676744Z",
     "start_time": "2021-05-06T03:46:03.165095Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# parameters setup\n",
    "params = {'subreddit': epl, 'size': '1', 'is_self': True}\n",
    "\n",
    "# assignment of function return to variable\n",
    "get_req = get_request(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.699385Z",
     "start_time": "2021-05-06T03:46:03.682464Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "subbreddit: PremierLeague\n",
      "     title: When will Premier League teams figure out Thomas T...\n",
      "    author: Lersbyte\n",
      "   created: 1620259116\n",
      "  comments: 0\n",
      "       url: https://www.reddit.com/r/PremierLeague/comments/n5uajf/when_will_premier_league_teams_figure_out_thomas/\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "f'''\n",
    "subbreddit: {get_req[0]['subreddit']}\n",
    "     title: {get_req[0]['title'][:50]}...\n",
    "    author: {get_req[0]['author']}\n",
    "   created: {get_req[0]['created_utc']}\n",
    "  comments: {get_req[0]['num_comments']}\n",
    "       url: {get_req[0]['url']}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Loop Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "I wanted to make the process of scraping as easy as possible, so I built a function that has evolved over the course of my work for OverArmor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.732027Z",
     "start_time": "2021-05-06T03:46:03.711384Z"
    },
    "code_folding": [
     21
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to make [n] requests of 100 posts. \n",
    "# Each request will be for a period of [win] days and the periods will not overlap\n",
    "# Exports .csv of API responses for each subreddit\n",
    "\n",
    "@knockknock.slack_sender(webhook_url=kk_url, channel=kk_channel_name, user_mentions=kk_users)\n",
    "def api_requests(n, win, subs_list, output_folder, size=100, is_self=True, is_video=False):\n",
    "\n",
    "    # For Loop to scrape multiple subreddits:\n",
    "    for sub in subs_list:\n",
    "        \n",
    "        # List instantiation for error/event logging\n",
    "        report = []\n",
    "        \n",
    "        # Onscreen display during loops:        \n",
    "        cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f'API request initiated. Scraping r/{sub} in progress...')\n",
    "        print(f'   Current time: {cur_datetime}')\n",
    "        print('')\n",
    "        \n",
    "        # Adding updates to event log\n",
    "        report.append({'event': f'API request from r/{sub} began',\n",
    "                       'datetime': f'{cur_datetime}',\n",
    "                       'exception': ''\n",
    "                       })\n",
    "\n",
    "        # Setting parameters to use in data requests from the API. Parameters mostly set by function args. \n",
    "        params = {\n",
    "            'subreddit': sub,\n",
    "            'size': size,\n",
    "            'is_self': is_self,\n",
    "            'is_video': is_video,\n",
    "            'selftext:not': '[removed]'      # thanks to Amanda for posting this in the groupwork channel\n",
    "        }\n",
    "\n",
    "        list_data = []\n",
    "        \n",
    "        for i in range(1, n+1):               # I used the demo notebook to figure out these time\n",
    "            params['after'] = f'{i * win}d'   # parameters and loop structure using n and day_window\n",
    "\n",
    "            # Try/Except to handle any errors in the get_requests\n",
    "            try:\n",
    "                new_data = get_request(params)\n",
    "\n",
    "            # Adding errors to event log if get_request fails\n",
    "            except Exception as e:                                              \n",
    "                cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                report.append({'event': f'REQUEST FAILED: {i}',\n",
    "                               'datetime': f'{cur_datetime}',\n",
    "                               'exception': f'{e}'\n",
    "                               })\n",
    "                \n",
    "                # Onscreen display - Failure of single request\n",
    "                print(f'Request failed: {i}')\n",
    "                print(f'  Current time: {cur_datetime}')\n",
    "                print('')\n",
    "                time.sleep(.25)\n",
    "                continue\n",
    "                \n",
    "            # Adding newly pulled data to list for eventual DataFrame conversion and export\n",
    "            list_data.extend(new_data)\n",
    "    \n",
    "            # Adding completion updates to event log\n",
    "            cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            report.append({'event': f'Request complete: {i}',\n",
    "                           'datetime': f'{cur_datetime}',\n",
    "                           'exception': ''\n",
    "                           })\n",
    "            \n",
    "            # Onscreen updates for every 10 requests to provide timing updates\n",
    "            if i % 5 == 0:\n",
    "                print(f'Request complete: {i}')\n",
    "                print(f'    Current time: {cur_datetime}')\n",
    "                print('')\n",
    "            df_report = pd.DataFrame(report)\n",
    "            df_report.to_csv(f'../git_ignore/output/report.csv', index=False)\n",
    "            time.sleep(.25)\n",
    "\n",
    "            \n",
    "        # Setting up list of desired features to keep in primary output. Others will be excluded:\n",
    "        features = ['subreddit', 'created_utc', 'author', 'num_comments',\n",
    "                    'score', 'is_self', 'link_flair_text','title', 'selftext', 'full_link']\n",
    "            \n",
    "        # creating two DataFrames from list_data after all requests have been made and completed \n",
    "        df_output = pd.DataFrame(list_data)\n",
    "        df_outputfull = pd.DataFrame(list_data)\n",
    "        \n",
    "        # filtering the primary DataFrame\n",
    "        df_output = df_output[features]\n",
    "        \n",
    "        # creating simple date and time columns using utc code\n",
    "        # https://docs.python.org/3/library/datetime.html#date-objects\n",
    "        # https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "        df_output['date'] = df_output['created_utc'].map(lambda x: dt.datetime.fromtimestamp(x).strftime('%Y-%m-%d'))\n",
    "        df_output['time'] = df_output['created_utc'].map(lambda x: dt.datetime.fromtimestamp(x).strftime('%H:%M:%S'))\n",
    "\n",
    "        # Exporting data with selected/filtered features\n",
    "        # using if/else statement to accomodate varied input formatting\n",
    "        df_output.reset_index(inplace=True)\n",
    "        cur_datetime = dt.datetime.now().strftime('%Y_%m_%d-%H_%M_%S')\n",
    "\n",
    "        if output_folder[-1] == '/':\n",
    "            df_output.to_csv(f'{output_folder}{cur_datetime}_data_{sub}.csv', index=False,)\n",
    "        else:\n",
    "            df_output.to_csv(f'{output_folder}/{cur_datetime}_data_{sub}.csv', index=False,)     \n",
    "            \n",
    "        # Adding last update and then exporting event log as a CSV report\n",
    "        # using if/else statement to accomodate varied input formatting\n",
    "        cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        report.append({'event': f'r/{sub} data files saved to output folder', 'datetime': f'{cur_datetime}'})\n",
    "\n",
    "        df_report = pd.DataFrame(report)\n",
    "        cur_datetime = dt.datetime.now().strftime('%Y_%m_%d-%H_%M_%S')\n",
    "        \n",
    "        if output_folder[-1] == '/':\n",
    "            df_report.to_csv(f'{output_folder}{cur_datetime}_report_{sub}.csv', index=False,)\n",
    "        else:\n",
    "            df_report.to_csv(f'{output_folder}/{cur_datetime}_report_{sub}.csv', index=False,)\n",
    "\n",
    "        report = []\n",
    "\n",
    "        # Onscreen display - completion of each subreddit scrape\n",
    "        cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f'r/{sub} data and report files saved to output folder')\n",
    "        print(f'   Current time: {cur_datetime}')\n",
    "        print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
